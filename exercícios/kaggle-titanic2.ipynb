{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aig033xOBStE",
        "outputId": "51633a86-625e-48a0-c922-26c4122dfc81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_decision_forests -U --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4WNqr4n9BuU",
        "outputId": "98c79f37-69a2-45f2-d1ae-bd501756af6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.  , 0.  , 0.  , 1.  , 0.41, 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ,\n",
              "       0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 1.  , 0.02, 0.75, 1.  , 0.84,\n",
              "       1.  , 0.95, 1.  , 0.  , 1.  , 1.  , 0.99, 0.  , 0.  , 0.  , 0.96,\n",
              "       0.  , 0.99, 0.96, 0.92, 0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 1.  ,\n",
              "       1.  , 0.  , 0.05, 0.  , 1.  , 0.95, 0.27, 0.  , 1.  , 1.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.92, 1.  ,\n",
              "       1.  , 0.  , 0.  , 1.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ,\n",
              "       1.  , 0.  , 1.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ,\n",
              "       1.  , 1.  , 0.  , 0.48, 1.  , 0.  , 0.07, 0.  , 1.  , 0.  , 0.  ,\n",
              "       0.  , 1.  , 0.  , 0.  , 0.  , 0.94, 0.  , 0.  , 0.  , 1.  , 0.  ,\n",
              "       0.  , 1.  , 1.  , 0.26, 1.  , 0.  , 0.  , 1.  , 0.06, 1.  , 1.  ,\n",
              "       0.  , 1.  , 0.  , 0.  , 1.  , 0.  , 1.  , 0.05, 0.  , 1.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ,\n",
              "       0.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 0.  , 1.  ,\n",
              "       0.  , 0.  , 1.  , 0.29, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.01,\n",
              "       0.01, 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ,\n",
              "       1.  , 1.  , 1.  , 1.  , 0.  , 0.14, 1.  , 0.  , 1.  , 0.  , 1.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.91, 0.  , 1.  , 0.  , 1.  , 0.  ,\n",
              "       0.  , 0.92, 1.  , 1.  , 0.02, 1.  , 0.  , 0.08, 0.  , 0.  , 1.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.22, 0.38, 0.  , 1.  , 0.  , 1.  , 0.  ,\n",
              "       1.  , 0.  , 1.  , 0.99, 0.96, 0.  , 0.  , 1.  , 0.  , 0.  , 0.  ,\n",
              "       1.  , 0.  , 0.  , 1.  , 0.  , 0.86, 1.  , 1.  , 1.  , 0.95, 1.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 1.  , 0.  , 0.01,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 1.  , 1.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.92, 0.  , 0.  , 0.  , 1.  , 1.  , 0.  ,\n",
              "       1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  , 0.  , 1.  , 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  , 1.  , 1.  ,\n",
              "       0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 1.  , 1.  , 0.  , 0.03, 0.  ,\n",
              "       0.  , 0.  , 0.  , 0.  , 1.  , 1.  , 0.  , 1.  , 0.  , 0.  , 0.  ,\n",
              "       1.  , 0.78, 0.  , 0.61, 0.  , 0.  , 0.92, 0.  , 0.  , 0.  , 0.  ,\n",
              "       0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 0.92, 0.  , 1.  , 1.  , 0.  ,\n",
              "       0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 0.  , 1.  , 0.  , 1.  , 1.  ,\n",
              "       1.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.98, 0.  , 1.  , 0.  , 0.  ,\n",
              "       1.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.01, 0.  ,\n",
              "       1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 1.  ,\n",
              "       0.  , 1.  , 0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 0.01, 0.54, 0.  ,\n",
              "       0.  , 1.  , 1.  , 1.  , 1.  , 0.  , 0.  , 1.  , 0.  , 0.  , 1.  ])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "titanic_test = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/datasets/titanic-test.csv\")\n",
        "titanic_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/datasets/titanic-train.csv\")\n",
        "\n",
        "# pré-processamento\n",
        "\n",
        "# name - normalização de texto (letras minúsculas), tokenização; ticket - separação das partes\n",
        "\n",
        "# preprocess() https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests\n",
        "# def preprocess(df):\n",
        "#   df = df.copy()\n",
        "#   def tokenize_name(name):\n",
        "#     name = name.lower()\n",
        "#     return \" \".join([one_name.strip(\",()[].\\\"'\") for one_name in name.split(\" \")])\n",
        "\n",
        "#   def ticket_number(x):\n",
        "#     return x.split(\" \")[-1]\n",
        "\n",
        "#   def ticket_item(x):\n",
        "#     items = x.split(\" \")\n",
        "#     if len(items) == 1:\n",
        "#       return \"NONE\"\n",
        "#     return \"_\".join(items[0:-1])\n",
        "#   df[\"Name\"] = df[\"Name\"].apply(tokenize_name)\n",
        "#   df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n",
        "#   df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)\n",
        "#   return df\n",
        "\n",
        "# titanic_train = preprocess(titanic_train)\n",
        "# titanic_test = preprocess(titanic_test)\n",
        "\n",
        "# sex - vamos converter para um valor numérico\n",
        "# 0 - female, 1 - male\n",
        "titanic_train['Sex'] = LabelEncoder().fit_transform(titanic_train['Sex'])\n",
        "titanic_test['Sex'] = LabelEncoder().fit_transform(titanic_test['Sex'])\n",
        "\n",
        "# cabin - tem muitos valores faltantes; solução escolhida: exclusão da coluna\n",
        "titanic_train = titanic_train.drop('Cabin', axis=1)\n",
        "titanic_test = titanic_test.drop('Cabin', axis=1)\n",
        "\n",
        "# embarked - deletamos as linhas em que há valor faltante em 'Embarked'\n",
        "titanic_train.dropna(subset=[\"Embarked\"])\n",
        "titanic_test.dropna(subset=[\"Embarked\"])\n",
        "\n",
        "# após isso, usamos a técnica de one hot encoded\n",
        "titanic_train = pd.concat([titanic_train, pd.get_dummies(titanic_train['Embarked'], prefix='Embarked')], axis=1)\n",
        "titanic_train = titanic_train.drop('Embarked', axis=1)\n",
        "titanic_test = pd.concat([titanic_test, pd.get_dummies(titanic_test['Embarked'], prefix='Embarked')], axis=1)\n",
        "titanic_test = titanic_test.drop('Embarked', axis=1)\n",
        "\n",
        "# age - substituímos os valores faltantes pelo valor da média de todas as idades\n",
        "titanic_train.fillna(titanic_train['Age'].median(), inplace=True)\n",
        "titanic_test.fillna(titanic_test['Age'].median(), inplace=True)\n",
        "\n",
        "'''\n",
        "treinamento do modelo\n",
        "\n",
        "Ideia: criar um ensemble, que é o uso de vários modelos\n",
        "Vários modelos são treinados e suas previsões são armazenadas; ao fim, as previsões são combinadas\n",
        "para produção de um resultado final robusto\n",
        "'''\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "titanic_train_prep = titanic_train.drop('Survived', axis=1)\n",
        "titanic_train_prep = titanic_train_prep.drop('PassengerId', axis=1)\n",
        "titanic_train_prep = titanic_train_prep.drop('Name', axis=1)\n",
        "titanic_train_prep = titanic_train_prep.drop('Ticket', axis=1)\n",
        "titanic_train_labels = titanic_train['Survived'].copy()\n",
        "\n",
        "titanic_test_model = titanic_test.drop('PassengerId', axis=1)\n",
        "titanic_test_model = titanic_test_model.drop('Name', axis=1)\n",
        "titanic_test_model = titanic_test_model.drop('Ticket', axis=1)\n",
        "\n",
        "pred = None\n",
        "num_pred = 0\n",
        "\n",
        "for i in range(100):\n",
        "  random_f = RandomForestClassifier(random_state=i)\n",
        "  random_f.fit(titanic_train_prep, titanic_train_labels)\n",
        "  # previsão do conjunto de teste\n",
        "  pred_test = random_f.predict(titanic_test_model)\n",
        "  # soma das previsões\n",
        "  if pred is None:\n",
        "      pred = pred_test\n",
        "  else:\n",
        "      pred += pred_test\n",
        "  num_pred += 1\n",
        "\n",
        "pred = pred.astype(float)\n",
        "pred /= 100.0\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "E49BKIp3pr-s"
      },
      "outputs": [],
      "source": [
        "my_submission = pd.DataFrame({'PassengerId': titanic_test[\"PassengerId\"], 'Survived': (pred >= 0.5).astype(int)})\n",
        "my_submission.to_csv('LudmilaSubmission2.csv', index=False)\n",
        "# kaggle score: 0.76315"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
